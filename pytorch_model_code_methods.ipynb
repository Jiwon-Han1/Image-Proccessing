{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구현 방식 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제시된 방법\n",
    "- 방법 1: **직접 nn.Module을 상속하여 구현** &rarr; 더 많은 제어 가능성\n",
    "- 방법 2: **nn.Sequential을 사용하여 구현** &rarr; 코드가 더 간단하고 유지보수가 용이\n",
    "- 방법 3: **리스트로 묶어 제시하는 방식** &rarr; 간결함과 유연성을 결합\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 직접 nn.Module을 상속하여 구현\n",
    "\n",
    "**장점**\n",
    "- 더 많은 제어 가능성: 각 레이어와 연산을 세밀하게 조작할 수 있습니다.\n",
    "- 디버깅 용이: 각 단계별로 출력을 확인하고 문제를 찾기 쉽습니다.\n",
    "\n",
    "**단점**\n",
    "- 코드가 더 길고 복잡해질 수 있음: 특히 레이어가 많을 경우 코드가 길어지고 복잡해질 수 있습니다.\n",
    "- 반복적인 코드: 레이어를 추가하거나 변경할 때 코드의 여러 부분을 수정해야 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CycleGAN Generator - Method 1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_features, in_features, kernel_size=3, padding=1)\n",
    "        self.in1 = nn.InstanceNorm2d(in_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_features, in_features, kernel_size=3, padding=1)\n",
    "        self.in2 = nn.InstanceNorm2d(in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.in1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.in2(x)\n",
    "        x += residual\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_nc, 64, kernel_size=7, padding=3)\n",
    "        self.in1 = nn.InstanceNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.in2 = nn.InstanceNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.in3 = nn.InstanceNorm2d(256)\n",
    "\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(256) for _ in range(n_residual_blocks)]\n",
    "        )\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.in4 = nn.InstanceNorm2d(128)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.in5 = nn.InstanceNorm2d(64)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, output_nc, kernel_size=7, padding=3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.in1(self.conv1(x)))\n",
    "        x = self.relu(self.in2(self.conv2(x)))\n",
    "        x = self.relu(self.in3(self.conv3(x)))\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.relu(self.in4(self.deconv1(x)))\n",
    "        x = self.relu(self.in5(self.deconv2(x)))\n",
    "        x = self.tanh(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "# Generator 모델 생성 및 요약\n",
    "input_nc = 3  # J, H, K 입력 채널\n",
    "output_nc = 1  # ch1 출력 채널\n",
    "generator = Generator(input_nc=input_nc, output_nc=output_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### 2. nn.Sequential을 사용하여 구현\n",
    "\n",
    "**장점**\n",
    "- 코드가 간결하고 이해하기 쉬움: 레이어들을 순서대로 정의하면 되므로 코드가 더 간결합니다.\n",
    "- 유지보수 용이: 레이어를 추가하거나 제거할 때 코드 수정이 간편합니다.\n",
    "\n",
    "**단점**\n",
    "- 제어가 제한적임: nn.Sequential을 사용하면 각 레이어 사이에 추가 연산을 삽입하기 어려워집니다.\n",
    "- 복잡한 네트워크 구성 시 어려움: 조건문이나 반복문을 사용하여 네트워크를 구성할 때 제한적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *[ResidualBlock(256) for _ in range(n_residual_blocks)],\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, output_nc, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Generator 모델 생성 및 요약\n",
    "input_nc = 3  # J, H, K 입력 채널\n",
    "output_nc = 1  # ch1 출력 채널\n",
    "generator = Generator(input_nc=input_nc, output_nc=output_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### 3. 리스트로 묶어 제시하는 방식\n",
    "\n",
    "**장점**\n",
    "- nn.Sequential의 간결함과 nn.Module의 유연성을 결합: 코드가 간결하면서도 유연한 제어가 가능합니다.\n",
    "- 반복적인 구조를 쉽게 정의: 여러 레이어를 반복적으로 정의할 때 유용합니다.\n",
    "\n",
    "**단점**\n",
    "- 초기 학습 곡선: 이 방식에 익숙해지기까지 시간이 걸릴 수 있습니다.\n",
    "- 디버깅 어려움: 리스트 내의 레이어들 사이에서 버그를 찾기 어려울 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CycleGAN Generator - Method 3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(input_nc, 64, kernel_size=7, padding=3))\n",
    "        layers.append(nn.InstanceNorm2d(64))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        layers.append(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1))\n",
    "        layers.append(nn.InstanceNorm2d(128))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        layers.append(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1))\n",
    "        layers.append(nn.InstanceNorm2d(256))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        for _ in range(n_residual_blocks):\n",
    "            layers.append(ResidualBlock(256))\n",
    "        \n",
    "        layers.append(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "        layers.append(nn.InstanceNorm2d(128))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        layers.append(nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "        layers.append(nn.InstanceNorm2d(64))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        layers.append(nn.Conv2d(64, output_nc, kernel_size=7, padding=3))\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Generator 모델 생성 및 요약\n",
    "input_nc = 3  # J, H, K 입력 채널\n",
    "output_nc = 1  # ch1 출력 채널\n",
    "generator = Generator(input_nc=input_nc, output_nc=output_nc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
